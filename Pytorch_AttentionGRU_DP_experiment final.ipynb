{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1TxOXSk7EByCs6bEZ_ObsNFBcrzrv4MXb",
      "authorship_tag": "ABX9TyNVfkV3668N74cE5N/Bb0r5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jingvf/IDS/blob/main/Pytorch_AttentionGRU_DP_experiment%20final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9826b547",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a7eb04e-802b-4728-e390-8e9ba615ee12"
      },
      "source": [
        "!pip install opacus==1.1.3 -q # 指定稳定版本\n",
        "\n",
        "import opacus\n",
        "print(opacus.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "S8Jmwbcv_DdW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea3aea08-a51a-4d33-df1a-387827c079b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from opacus import PrivacyEngine\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "# 加载数据\n",
        "data_path = '/content/drive/MyDrive/2025paper/dataset/small_dataset.csv'  # 替换为你的数据路径\n",
        "df = pd.read_csv(data_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 输出类别数量\n",
        "num_classes = df['Class'].nunique()\n",
        "print(f\"总类别数: {num_classes}\")\n",
        "\n",
        "# 输出每个类别的数量\n",
        "class_counts = df['Class'].value_counts()\n",
        "print(\"每个类别的样本数量：\")\n",
        "print(class_counts)\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "EO6bYi9E_3Ut",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5981926a-3595-42b6-ffcb-03fef0a6c98a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "总类别数: 5\n",
            "每个类别的样本数量：\n",
            "Class\n",
            "Normal          946844\n",
            "SpoofingRPM      65490\n",
            "SpoofingGear     59725\n",
            "DoS              10553\n",
            "Fuzzy             8967\n",
            "Name: count, dtype: int64\n",
            "      Timestamp CAN ID  DLC Data[0] Data[1] Data[2] Data[3] Data[4] Data[5]  \\\n",
            "0  1.478192e+09    316    8      45      29      24      ff      29      24   \n",
            "1  1.478192e+09    316    8      45      29      24      ff      29      24   \n",
            "2  1.478195e+09   0140    8      00      00      00      00      08      28   \n",
            "3  1.478191e+09   0545    8      d8      00      00      8a      00      00   \n",
            "4  1.478195e+09   043f    8       1      45      60      ff      6b       0   \n",
            "\n",
            "  Data[6] Data[7]         Class  \n",
            "0       0      ff   SpoofingRPM  \n",
            "1       0      ff   SpoofingRPM  \n",
            "2      2f      15        Normal  \n",
            "3      00      00        Normal  \n",
            "4       0       0  SpoofingGear  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. CAN ID 转成 int（十六进制字符串 → 十进制）\n",
        "# =====================\n",
        "df[\"CAN ID\"] = df[\"CAN ID\"].apply(lambda x: int(str(x), 16) if isinstance(x, str) else int(x))\n",
        "\n",
        "# 特征列\n",
        "feature_cols = [\"CAN ID\", \"DLC\"] + [f\"Data[{i}]\" for i in range(8)]\n",
        "X_features = df[feature_cols].astype(str).applymap(lambda x: int(x, 16) if isinstance(x, str) else int(x)).values\n",
        "\n",
        "# 标签\n",
        "y = df[\"Class\"].values\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "# 标准化\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_features)\n",
        "\n",
        "# 数据维度调整 (B, T, F)，这里把 T=1 表示每条 CAN 消息为一个时间步\n",
        "X_scaled = X_scaled[:, np.newaxis, :]  # shape -> [num_samples, 1, num_features]\n",
        "\n",
        "# =====================\n",
        "# 5. 类别权重 (用统计的数量，如果需要自动算就用 np.bincount(y_encoded))\n",
        "# =====================\n",
        "counts = [946844, 65490, 59725, 10553, 8967]  # 示例：之前统计好的数量\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ],
      "metadata": {
        "id": "A0HmSkdw_2CI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beb59ffd-8f18-4b08-cdcf-987e1d151b68"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1351706014.py:7: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  X_features = df[feature_cols].astype(str).applymap(lambda x: int(x, 16) if isinstance(x, str) else int(x)).values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y_encoded, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# 转 Tensor\n",
        "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n",
        "                              torch.tensor(y_train, dtype=torch.long))\n",
        "val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32),\n",
        "                            torch.tensor(y_val, dtype=torch.long))\n",
        "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
        "                             torch.tensor(y_test, dtype=torch.long))\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "AH38IIR1TFrs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. 模型定义：CNN-Attention (DP友好)\n",
        "# ================================\n",
        "class DPMultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        Q = self.q_proj(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = self.k_proj(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v_proj(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        attn_scores = (Q @ K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
        "        attn_out = attn_weights @ V\n",
        "        attn_out = attn_out.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        return self.out_proj(attn_out)\n",
        "\n",
        "class DPCNNAttention(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes, num_heads=4, hidden_dim=128):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(input_dim, hidden_dim, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
        "        self.gn1 = nn.GroupNorm(num_groups=8, num_channels=hidden_dim)\n",
        "        self.gn2 = nn.GroupNorm(num_groups=8, num_channels=hidden_dim)\n",
        "        self.attn = DPMultiHeadSelfAttention(hidden_dim, num_heads)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)\n",
        "        x = F.relu(self.gn1(self.conv1(x)))\n",
        "        x = F.relu(self.gn2(self.conv2(x)))\n",
        "        x = x.transpose(1, 2)\n",
        "        attn_out = self.attn(x)\n",
        "        pooled = attn_out.mean(dim=1)\n",
        "        return self.fc(pooled)"
      ],
      "metadata": {
        "id": "6marWhK6T4DV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DPCNNAttention(11, num_classes).to(device)\n",
        "\n",
        "from torchsummary import summary\n",
        "summary(model, input_size=(100, 11))  # 序列长度为10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJWW9QrpjNQF",
        "outputId": "0069f2f0-49ef-481a-d7dc-78d4ccc79279"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv1d-1             [-1, 128, 100]           4,352\n",
            "         GroupNorm-2             [-1, 128, 100]             256\n",
            "            Conv1d-3             [-1, 128, 100]          49,280\n",
            "         GroupNorm-4             [-1, 128, 100]             256\n",
            "            Linear-5             [-1, 100, 128]          16,512\n",
            "            Linear-6             [-1, 100, 128]          16,512\n",
            "            Linear-7             [-1, 100, 128]          16,512\n",
            "            Linear-8             [-1, 100, 128]          16,512\n",
            "DPMultiHeadSelfAttention-9             [-1, 100, 128]               0\n",
            "           Linear-10                    [-1, 5]             645\n",
            "================================================================\n",
            "Total params: 120,837\n",
            "Trainable params: 120,837\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.88\n",
            "Params size (MB): 0.46\n",
            "Estimated Total Size (MB): 1.34\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from opacus import PrivacyEngine\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "\n",
        "def run_dp_comparison(train_loader, val_loader, test_loader,\n",
        "                      input_dim, num_classes, target_epsilons=[1,3,5],\n",
        "                      epochs=20, device=\"cuda\"):\n",
        "\n",
        "    results = {\"SGD\": {}, \"Adam\": {}}\n",
        "\n",
        "    for eps in target_epsilons:\n",
        "        for opt_name in [\"SGD\", \"Adam\"]:\n",
        "            print(f\"\\nRunning {opt_name} with target ε={eps}\")\n",
        "\n",
        "            model = DPCNNAttention(input_dim=input_dim, num_classes=num_classes).to(device)\n",
        "            optimizer = torch.optim.SGD(model.parameters(), lr=1e-3) if opt_name==\"SGD\" else torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "            privacy_engine = PrivacyEngine()\n",
        "            model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
        "                module=model,\n",
        "                optimizer=optimizer,\n",
        "                data_loader=train_loader,\n",
        "                epochs=epochs,\n",
        "                target_epsilon=eps,\n",
        "                target_delta=1e-5,\n",
        "                max_grad_norm=1.0,\n",
        "            )\n",
        "\n",
        "            train_losses, val_accs, epsilons_list = [], [], []\n",
        "\n",
        "            for epoch in range(epochs):\n",
        "                model.train()\n",
        "                total_loss, total_correct, total_samples = 0, 0, 0\n",
        "                for x, y in train_loader:\n",
        "                    x, y = x.to(device), y.to(device)\n",
        "                    optimizer.zero_grad()\n",
        "                    out = model(x)\n",
        "                    loss = criterion(out, y)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    total_loss += loss.item() * y.size(0)\n",
        "                    total_correct += (out.argmax(1)==y).sum().item()\n",
        "                    total_samples += y.size(0)\n",
        "                train_losses.append(total_loss/total_samples)\n",
        "\n",
        "                # Val\n",
        "                model.eval()\n",
        "                val_correct, val_total = 0,0\n",
        "                with torch.no_grad():\n",
        "                    for x, y in val_loader:\n",
        "                        x, y = x.to(device), y.to(device)\n",
        "                        out = model(x)\n",
        "                        val_correct += (out.argmax(1)==y).sum().item()\n",
        "                        val_total += y.size(0)\n",
        "                val_acc = val_correct / val_total\n",
        "                val_accs.append(val_acc)\n",
        "\n",
        "                eps_curr = privacy_engine.accountant.get_epsilon(delta=1e-5)\n",
        "                epsilons_list.append(eps_curr)\n",
        "\n",
        "                print(f\"[{opt_name}] Epoch {epoch+1}/{epochs}: Loss={train_losses[-1]:.4f}, Val Acc={val_acc:.4f}, ε={eps_curr:.2f}\")\n",
        "\n",
        "            # Test\n",
        "            model.eval()\n",
        "            y_true, y_pred = [], []\n",
        "            with torch.no_grad():\n",
        "                for x, y in test_loader:\n",
        "                    x, y = x.to(device), y.to(device)\n",
        "                    out = model(x)\n",
        "                    y_true.extend(y.cpu().numpy())\n",
        "                    y_pred.extend(out.argmax(1).cpu().numpy())\n",
        "            conf_mat = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "            results[opt_name][eps] = {\n",
        "                \"train_losses\": train_losses,\n",
        "                \"val_accs\": val_accs,\n",
        "                \"epsilons\": epsilons_list,\n",
        "                \"conf_mat\": conf_mat,\n",
        "                \"model\": model  # 保存最终模型\n",
        "            }\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "oWOlVs5HaIzQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1️⃣ 准备 DataLoader（train_loader, val_loader, test_loader）和参数\n",
        "input_dim = X_scaled.shape[2]  # 特征维度\n",
        "num_classes = len(set(y_encoded))\n",
        "target_epsilons = [1, 3, 5, 7]  # 你想对比的 epsilon 值\n",
        "epochs = 10\n",
        "\n",
        "# 2️⃣ 运行 DP-SGD / DP-Adam 对比实验，得到 results\n",
        "results = run_dp_comparison(train_loader, val_loader, test_loader,\n",
        "                             input_dim=input_dim,\n",
        "                             num_classes=num_classes,\n",
        "                             target_epsilons=target_epsilons,\n",
        "                             epochs=epochs,\n",
        "                             device=device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Jmx6U1O6bdw",
        "outputId": "61810bb1-eb07-404b-b02d-59b582ddb305"
      },
      "execution_count": 15,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/opacus/privacy_engine.py:133: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "WARNING:opacus.data_loader:Ignoring drop_last as it is not compatible with DPDataLoader.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Running SGD with target ε=1\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1864: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SGD] Epoch 1/10: Loss=0.6813, Val Acc=0.8678, ε=0.90\n",
            "[SGD] Epoch 2/10: Loss=0.4680, Val Acc=0.8678, ε=0.92\n",
            "[SGD] Epoch 3/10: Loss=0.3125, Val Acc=0.9278, ε=0.93\n",
            "[SGD] Epoch 4/10: Loss=0.2321, Val Acc=0.9782, ε=0.94\n",
            "[SGD] Epoch 5/10: Loss=0.1845, Val Acc=0.9762, ε=0.95\n",
            "[SGD] Epoch 6/10: Loss=0.1659, Val Acc=0.9749, ε=0.96\n",
            "[SGD] Epoch 7/10: Loss=0.1632, Val Acc=0.9751, ε=0.97\n",
            "[SGD] Epoch 8/10: Loss=0.1577, Val Acc=0.9756, ε=0.98\n",
            "[SGD] Epoch 9/10: Loss=0.1539, Val Acc=0.9762, ε=0.99\n",
            "[SGD] Epoch 10/10: Loss=0.1479, Val Acc=0.9765, ε=0.99\n",
            "\n",
            "Running Adam with target ε=1\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/opacus/privacy_engine.py:133: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1864: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Adam] Epoch 1/10: Loss=0.0630, Val Acc=0.9963, ε=0.90\n",
            "[Adam] Epoch 2/10: Loss=0.0159, Val Acc=0.9970, ε=0.92\n",
            "[Adam] Epoch 3/10: Loss=0.0171, Val Acc=0.9973, ε=0.93\n",
            "[Adam] Epoch 4/10: Loss=0.0182, Val Acc=0.9977, ε=0.94\n",
            "[Adam] Epoch 5/10: Loss=0.0193, Val Acc=0.9970, ε=0.95\n",
            "[Adam] Epoch 6/10: Loss=0.0183, Val Acc=0.9970, ε=0.96\n",
            "[Adam] Epoch 7/10: Loss=0.0192, Val Acc=0.9974, ε=0.97\n",
            "[Adam] Epoch 8/10: Loss=0.0184, Val Acc=0.9971, ε=0.98\n",
            "[Adam] Epoch 9/10: Loss=0.0151, Val Acc=0.9971, ε=0.99\n",
            "[Adam] Epoch 10/10: Loss=0.0156, Val Acc=0.9973, ε=0.99\n",
            "\n",
            "Running SGD with target ε=3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/opacus/privacy_engine.py:133: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1864: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SGD] Epoch 1/10: Loss=0.6493, Val Acc=0.8678, ε=2.47\n",
            "[SGD] Epoch 2/10: Loss=0.4135, Val Acc=0.9278, ε=2.58\n",
            "[SGD] Epoch 3/10: Loss=0.2746, Val Acc=0.9822, ε=2.66\n",
            "[SGD] Epoch 4/10: Loss=0.2094, Val Acc=0.9772, ε=2.73\n",
            "[SGD] Epoch 5/10: Loss=0.1746, Val Acc=0.9756, ε=2.78\n",
            "[SGD] Epoch 6/10: Loss=0.1692, Val Acc=0.9749, ε=2.83\n",
            "[SGD] Epoch 7/10: Loss=0.1630, Val Acc=0.9756, ε=2.88\n",
            "[SGD] Epoch 8/10: Loss=0.1552, Val Acc=0.9765, ε=2.92\n",
            "[SGD] Epoch 9/10: Loss=0.1480, Val Acc=0.9770, ε=2.96\n",
            "[SGD] Epoch 10/10: Loss=0.1440, Val Acc=0.9775, ε=2.99\n",
            "\n",
            "Running Adam with target ε=3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/opacus/privacy_engine.py:133: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1864: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Adam] Epoch 1/10: Loss=0.0537, Val Acc=0.9969, ε=2.47\n",
            "[Adam] Epoch 2/10: Loss=0.0157, Val Acc=0.9976, ε=2.58\n",
            "[Adam] Epoch 3/10: Loss=0.0150, Val Acc=0.9979, ε=2.66\n",
            "[Adam] Epoch 4/10: Loss=0.0152, Val Acc=0.9978, ε=2.73\n",
            "[Adam] Epoch 5/10: Loss=0.0148, Val Acc=0.9983, ε=2.78\n",
            "[Adam] Epoch 6/10: Loss=0.0152, Val Acc=0.9979, ε=2.83\n",
            "[Adam] Epoch 7/10: Loss=0.0161, Val Acc=0.9984, ε=2.88\n",
            "[Adam] Epoch 8/10: Loss=0.0139, Val Acc=0.9986, ε=2.92\n",
            "[Adam] Epoch 9/10: Loss=0.0134, Val Acc=0.9985, ε=2.96\n",
            "[Adam] Epoch 10/10: Loss=0.0144, Val Acc=0.9981, ε=2.99\n",
            "\n",
            "Running SGD with target ε=5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/opacus/privacy_engine.py:133: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1864: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SGD] Epoch 1/10: Loss=0.7190, Val Acc=0.8678, ε=3.77\n",
            "[SGD] Epoch 2/10: Loss=0.4929, Val Acc=0.8678, ε=4.02\n",
            "[SGD] Epoch 3/10: Loss=0.3256, Val Acc=0.9278, ε=4.21\n",
            "[SGD] Epoch 4/10: Loss=0.2409, Val Acc=0.9786, ε=4.35\n",
            "[SGD] Epoch 5/10: Loss=0.1971, Val Acc=0.9769, ε=4.49\n",
            "[SGD] Epoch 6/10: Loss=0.1773, Val Acc=0.9761, ε=4.60\n",
            "[SGD] Epoch 7/10: Loss=0.1656, Val Acc=0.9761, ε=4.71\n",
            "[SGD] Epoch 8/10: Loss=0.1611, Val Acc=0.9768, ε=4.82\n",
            "[SGD] Epoch 9/10: Loss=0.1545, Val Acc=0.9774, ε=4.91\n",
            "[SGD] Epoch 10/10: Loss=0.1484, Val Acc=0.9780, ε=5.00\n",
            "\n",
            "Running Adam with target ε=5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/opacus/privacy_engine.py:133: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1864: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Adam] Epoch 1/10: Loss=0.0482, Val Acc=0.9967, ε=3.77\n",
            "[Adam] Epoch 2/10: Loss=0.0148, Val Acc=0.9977, ε=4.02\n",
            "[Adam] Epoch 3/10: Loss=0.0146, Val Acc=0.9978, ε=4.21\n",
            "[Adam] Epoch 4/10: Loss=0.0142, Val Acc=0.9980, ε=4.35\n",
            "[Adam] Epoch 5/10: Loss=0.0143, Val Acc=0.9979, ε=4.49\n",
            "[Adam] Epoch 6/10: Loss=0.0136, Val Acc=0.9975, ε=4.60\n",
            "[Adam] Epoch 7/10: Loss=0.0136, Val Acc=0.9980, ε=4.71\n",
            "[Adam] Epoch 8/10: Loss=0.0145, Val Acc=0.9984, ε=4.82\n",
            "[Adam] Epoch 9/10: Loss=0.0151, Val Acc=0.9985, ε=4.91\n",
            "[Adam] Epoch 10/10: Loss=0.0146, Val Acc=0.9987, ε=5.00\n",
            "\n",
            "Running SGD with target ε=7\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/opacus/privacy_engine.py:133: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1864: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SGD] Epoch 1/10: Loss=0.6757, Val Acc=0.8678, ε=4.95\n",
            "[SGD] Epoch 2/10: Loss=0.4756, Val Acc=0.8678, ε=5.36\n",
            "[SGD] Epoch 3/10: Loss=0.3165, Val Acc=0.9278, ε=5.65\n",
            "[SGD] Epoch 4/10: Loss=0.2298, Val Acc=0.9756, ε=5.91\n",
            "[SGD] Epoch 5/10: Loss=0.1864, Val Acc=0.9741, ε=6.12\n",
            "[SGD] Epoch 6/10: Loss=0.1711, Val Acc=0.9732, ε=6.33\n",
            "[SGD] Epoch 7/10: Loss=0.1695, Val Acc=0.9735, ε=6.51\n",
            "[SGD] Epoch 8/10: Loss=0.1662, Val Acc=0.9741, ε=6.67\n",
            "[SGD] Epoch 9/10: Loss=0.1635, Val Acc=0.9746, ε=6.83\n",
            "[SGD] Epoch 10/10: Loss=0.1590, Val Acc=0.9751, ε=6.99\n",
            "\n",
            "Running Adam with target ε=7\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/opacus/privacy_engine.py:133: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1864: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Adam] Epoch 1/10: Loss=0.0479, Val Acc=0.9972, ε=4.95\n",
            "[Adam] Epoch 2/10: Loss=0.0162, Val Acc=0.9971, ε=5.36\n",
            "[Adam] Epoch 3/10: Loss=0.0148, Val Acc=0.9978, ε=5.65\n",
            "[Adam] Epoch 4/10: Loss=0.0129, Val Acc=0.9986, ε=5.91\n",
            "[Adam] Epoch 5/10: Loss=0.0116, Val Acc=0.9981, ε=6.12\n",
            "[Adam] Epoch 6/10: Loss=0.0119, Val Acc=0.9976, ε=6.33\n",
            "[Adam] Epoch 7/10: Loss=0.0126, Val Acc=0.9983, ε=6.51\n",
            "[Adam] Epoch 8/10: Loss=0.0125, Val Acc=0.9986, ε=6.67\n",
            "[Adam] Epoch 9/10: Loss=0.0123, Val Acc=0.9984, ε=6.83\n",
            "[Adam] Epoch 10/10: Loss=0.0125, Val Acc=0.9984, ε=6.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1️⃣ 可视化函数\n",
        "# ================================\n",
        "def plot_epochs_curves(results, epsilon):\n",
        "    optimizers = [\"SGD\", \"Adam\"]\n",
        "    plt.figure(figsize=(12,5))\n",
        "\n",
        "    # Loss\n",
        "    plt.subplot(1,2,1)\n",
        "    for opt in optimizers:\n",
        "        res = results[opt][epsilon]\n",
        "        epochs = range(1, len(res[\"train_losses\"])+1)\n",
        "        plt.plot(epochs, res[\"train_losses\"], label=f\"{opt} Train Loss\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\")\n",
        "    plt.title(f\"Training Loss per Epoch (ε={epsilon})\")\n",
        "    plt.legend()\n",
        "\n",
        "    # Val Accuracy\n",
        "    plt.subplot(1,2,2)\n",
        "    for opt in optimizers:\n",
        "        res = results[opt][epsilon]\n",
        "        epochs = range(1, len(res[\"val_accs\"])+1)\n",
        "        plt.plot(epochs, res[\"val_accs\"], label=f\"{opt} Val Acc\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Validation Accuracy\")\n",
        "    plt.title(f\"Validation Accuracy per Epoch (ε={epsilon})\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def plot_epsilon_comparison(results, metric=\"val_accs\"):\n",
        "    optimizers = [\"SGD\", \"Adam\"]\n",
        "    epsilons = sorted(list(results[\"SGD\"].keys()))\n",
        "\n",
        "    plt.figure(figsize=(12,5))\n",
        "\n",
        "    for opt in optimizers:\n",
        "        metric_vals = []\n",
        "        for eps in epsilons:\n",
        "            res = results[opt][eps]\n",
        "            if metric==\"epsilons\":\n",
        "                metric_vals.append(res[\"epsilons\"][-1])\n",
        "            else:\n",
        "                metric_vals.append(res[metric][-1])\n",
        "        plt.plot(epsilons, metric_vals, marker='o', label=f\"{opt}\")\n",
        "\n",
        "    plt.xlabel(\"Target ε\")\n",
        "    ylabel = \"Value\"\n",
        "    if metric==\"train_losses\":\n",
        "        ylabel = \"Train Loss\"\n",
        "    elif metric==\"val_accs\":\n",
        "        ylabel = \"Validation Accuracy\"\n",
        "    elif metric==\"epsilons\":\n",
        "        ylabel = \"Achieved Epsilon\"\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.title(f\"{ylabel} vs Target Epsilon\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_confusion_matrix(conf_mat, class_names, title=\"Confusion Matrix\"):\n",
        "    plt.figure(figsize=(6,5))\n",
        "    sns.heatmap(conf_mat, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "08IXOfzAmFHK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 3️⃣ 可视化某个 epsilon 下的训练曲线\n",
        "plot_epochs_curves(results, epsilon=5)\n",
        "\n",
        "# 4️⃣ 可视化不同 epsilon 下最终 Val Acc / Loss / Achieved ε\n",
        "plot_epsilon_comparison(results, metric=\"val_accs\")\n",
        "plot_epsilon_comparison(results, metric=\"train_losses\")\n",
        "plot_epsilon_comparison(results, metric=\"epsilons\")\n",
        "\n",
        "# 5️⃣ 混淆矩阵\n",
        "class_names = le.classes_  # 你的类别名\n",
        "plot_confusion_matrix(results[\"SGD\"][5][\"conf_mat\"], class_names, title=\"DP-SGD Confusion Matrix\")\n",
        "plot_confusion_matrix(results[\"Adam\"][5][\"conf_mat\"], class_names, title=\"DP-Adam Confusion Matrix\")\n"
      ],
      "metadata": {
        "id": "8f8d7C1O9F6u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}