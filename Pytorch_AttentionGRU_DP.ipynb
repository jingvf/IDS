{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1_WkuceyHm0bjqSJR562s3BgXW_nFw6Gp",
      "authorship_tag": "ABX9TyMRj16KanPChvCyIhs35kPQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jingvf/IDS/blob/main/Pytorch_AttentionGRU_DP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "S8Jmwbcv_DdW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 加载数据\n",
        "data_path = '/content/drive/MyDrive/2025paper/dataset/small_dataset.csv'  # 替换为你的数据路径\n",
        "df = pd.read_csv(data_path)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9826b547",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "320bbda3-4ac3-493c-8a4d-bc3bd69464c6"
      },
      "source": [
        "!pip install opacus -qq"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/254.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.4/254.4 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 输出类别数量\n",
        "num_classes = df['Class'].nunique()\n",
        "print(f\"总类别数: {num_classes}\")\n",
        "\n",
        "# 输出每个类别的数量\n",
        "class_counts = df['Class'].value_counts()\n",
        "print(\"每个类别的样本数量：\")\n",
        "print(class_counts)\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "EO6bYi9E_3Ut",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d200162-95de-4189-ad2d-3935ecd095a4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "总类别数: 5\n",
            "每个类别的样本数量：\n",
            "Class\n",
            "Normal          946844\n",
            "SpoofingRPM      65490\n",
            "SpoofingGear     59725\n",
            "DoS              10553\n",
            "Fuzzy             8967\n",
            "Name: count, dtype: int64\n",
            "      Timestamp CAN ID  DLC Data[0] Data[1] Data[2] Data[3] Data[4] Data[5]  \\\n",
            "0  1.478192e+09    316    8      45      29      24      ff      29      24   \n",
            "1  1.478192e+09    316    8      45      29      24      ff      29      24   \n",
            "2  1.478195e+09   0140    8      00      00      00      00      08      28   \n",
            "3  1.478191e+09   0545    8      d8      00      00      8a      00      00   \n",
            "4  1.478195e+09   043f    8       1      45      60      ff      6b       0   \n",
            "\n",
            "  Data[6] Data[7]         Class  \n",
            "0       0      ff   SpoofingRPM  \n",
            "1       0      ff   SpoofingRPM  \n",
            "2      2f      15        Normal  \n",
            "3      00      00        Normal  \n",
            "4       0       0  SpoofingGear  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 1. CAN ID 转成 int（十六进制字符串转十进制数字）\n",
        "df[\"CAN ID\"] = df[\"CAN ID\"].apply(lambda x: int(str(x), 16) if isinstance(x, str) else int(x))\n",
        "\n",
        "# 2. 提取数值特征 (DLC + Data[0-7]) 并转为十进制\n",
        "feature_cols = [\"DLC\"] + [f\"Data[{i}]\" for i in range(8)]\n",
        "# Use .map instead of .applymap for consistency, although applymap was the one triggering the warning\n",
        "X_features = df[feature_cols].astype(str).map(lambda x: int(x, 16) if isinstance(x, str) else int(x)).values\n",
        "\n",
        "# 3. 标准化\n",
        "scaler = StandardScaler()\n",
        "X_features_scaled = scaler.fit_transform(X_features)\n",
        "\n",
        "# 4. CAN ID 单独提取\n",
        "X_canid = df[\"CAN ID\"].values\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(df[\"Class\"].values)\n",
        "# Calculate class weights\n",
        "import numpy as np\n",
        "\n",
        "counts = [946844, 65490, 59725, 10553, 8967] # Using the counts from the previous output\n",
        "total = sum(counts)\n",
        "class_weights = [total/c for c in counts]\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)"
      ],
      "metadata": {
        "id": "A0HmSkdw_2CI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3df7188a-4113-425c-e41d-cc869eb78dec"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ===== 第一步：划分训练集 + 临时集（验证+测试） =====\n",
        "X_train_cid, X_temp_cid, X_train_feat, X_temp_feat, y_train, y_temp = train_test_split(\n",
        "    X_canid, X_features_scaled, y_encoded, test_size=0.3, random_state=42\n",
        ")\n",
        "# 这里 test_size=0.3 表示 30% 留作验证+测试\n",
        "\n",
        "# ===== 第二步：从临时集划分验证集和测试集 =====\n",
        "X_val_cid, X_test_cid, X_val_feat, X_test_feat, y_val, y_test = train_test_split(\n",
        "    X_temp_cid, X_temp_feat, y_temp, test_size=0.5, random_state=42\n",
        ")\n",
        "# test_size=0.5 表示临时集一半做测试，一半做验证\n",
        "\n",
        "# ===== 输出检查 =====\n",
        "print(\"原始数据：\")\n",
        "print(f\"X_canid shape = {X_canid.shape}\")\n",
        "print(f\"X_features_scaled shape = {X_features_scaled.shape}\")\n",
        "print(f\"y_encoded shape = {y_encoded.shape}\")\n",
        "\n",
        "print(\"\\n训练集：\")\n",
        "print(f\"CAN IDs shape = {X_train_cid.shape}\")\n",
        "print(f\"Features shape = {X_train_feat.shape}\")\n",
        "print(f\"Labels shape = {y_train.shape}\")\n",
        "\n",
        "print(\"\\n验证集：\")\n",
        "print(f\"CAN IDs shape = {X_val_cid.shape}\")\n",
        "print(f\"Features shape = {X_val_feat.shape}\")\n",
        "print(f\"Labels shape = {y_val.shape}\")\n",
        "\n",
        "print(\"\\n测试集：\")\n",
        "print(f\"CAN IDs shape = {X_test_cid.shape}\")\n",
        "print(f\"Features shape = {X_test_feat.shape}\")\n",
        "print(f\"Labels shape = {y_test.shape}\")\n",
        "\n",
        "print(\"\\n类别映射：\")\n",
        "print(dict(zip(le.classes_, le.transform(le.classes_))))\n"
      ],
      "metadata": {
        "id": "SnhzQDgAAT9Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c229dd3c-a176-4aae-9259-f11f11a23a71"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "原始数据：\n",
            "X_canid shape = (1091579,)\n",
            "X_features_scaled shape = (1091579, 9)\n",
            "y_encoded shape = (1091579,)\n",
            "\n",
            "训练集：\n",
            "CAN IDs shape = (764105,)\n",
            "Features shape = (764105, 9)\n",
            "Labels shape = (764105,)\n",
            "\n",
            "验证集：\n",
            "CAN IDs shape = (163737,)\n",
            "Features shape = (163737, 9)\n",
            "Labels shape = (163737,)\n",
            "\n",
            "测试集：\n",
            "CAN IDs shape = (163737,)\n",
            "Features shape = (163737, 9)\n",
            "Labels shape = (163737,)\n",
            "\n",
            "类别映射：\n",
            "{'DoS': np.int64(0), 'Fuzzy': np.int64(1), 'Normal': np.int64(2), 'SpoofingGear': np.int64(3), 'SpoofingRPM': np.int64(4)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opacus==1.1.3 -q # 指定稳定版本\n",
        "import opacus\n",
        "print(opacus.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2Ll7Z8fTD8c",
        "outputId": "17315877-2d20-438e-facc-4fe6fe2a5530"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/181.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.7/181.7 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h1.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# ======================\n",
        "# 1. 数据集定义\n",
        "# ======================\n",
        "SEQ_LEN = 5\n",
        "\n",
        "class CANSequenceDataset(Dataset):\n",
        "    def __init__(self, canid, features, labels, seq_len=10):\n",
        "        self.seq_len = seq_len\n",
        "        self.canid = torch.tensor(canid, dtype=torch.long)\n",
        "        self.features = torch.tensor(features, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels) - self.seq_len + 1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq_canid = self.canid[idx:idx+self.seq_len].clone().detach()\n",
        "        seq_features = self.features[idx:idx+self.seq_len].clone().detach()\n",
        "        label = self.labels[idx+self.seq_len-1].clone().detach()\n",
        "        return seq_canid, seq_features, label"
      ],
      "metadata": {
        "id": "ABr4dGt-B-hs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 2048*4\n",
        "\n",
        "train_dataset = CANSequenceDataset(X_train_cid, X_train_feat, y_train, seq_len=SEQ_LEN)\n",
        "val_dataset   = CANSequenceDataset(X_val_cid, X_val_feat, y_val, seq_len=SEQ_LEN)\n",
        "test_dataset  = CANSequenceDataset(X_test_cid, X_test_feat, y_test, seq_len=SEQ_LEN)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
        "\n",
        "\n",
        "# 打印一个 batch 的 shape\n",
        "for batch in train_loader:\n",
        "    cids, feats, labels = batch\n",
        "    print(\"Batch CAN ID shape:\", cids.shape)       # (BATCH_SIZE, SEQ_LEN)\n",
        "    print(\"Batch Features shape:\", feats.shape)    # (BATCH_SIZE, SEQ_LEN, 9)\n",
        "    print(\"Batch Labels shape:\", labels.shape)     # (BATCH_SIZE,)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFPNkVjONzr5",
        "outputId": "43a58c16-9401-4ec2-8c50-e8d1ee0cbddb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch CAN ID shape: torch.Size([8192, 5])\n",
            "Batch Features shape: torch.Size([8192, 5, 9])\n",
            "Batch Labels shape: torch.Size([8192])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from opacus import PrivacyEngine\n",
        "from opacus.validators import ModuleValidator\n",
        "from opacus.layers import DPGRU  # 导入Opacus兼容的GRU层\n",
        "\n",
        "class CNN_DPGRU_Model(nn.Module):\n",
        "    def __init__(self, num_features, hidden_dim, num_classes,\n",
        "                 cnn_channels=64, kernel_size=3, num_layers=1, dropout=0.3):\n",
        "        super(CNN_DPGRU_Model, self).__init__()\n",
        "\n",
        "        # CNN部分 (仍然是普通的Conv1d，梯度会由PrivacyEngine控制)\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=num_features, out_channels=cnn_channels,\n",
        "                     kernel_size=kernel_size, padding=1),\n",
        "            nn.ReLU(),\n",
        "            # Replace BatchNorm1d with GroupNorm\n",
        "            nn.GroupNorm(num_groups=8, num_channels=cnn_channels), # Using GroupNorm with 8 groups\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Conv1d(in_channels=cnn_channels, out_channels=cnn_channels,\n",
        "                     kernel_size=kernel_size, padding=1),\n",
        "            nn.ReLU(),\n",
        "            # Replace BatchNorm1d with GroupNorm\n",
        "            nn.GroupNorm(num_groups=8, num_channels=cnn_channels), # Using GroupNorm with 8 groups\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "\n",
        "        # DPGRU部分\n",
        "        self.gru = DPGRU(\n",
        "            input_size=cnn_channels,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True  # 双向\n",
        "        )\n",
        "\n",
        "        # 分类层\n",
        "        self.fc = nn.Linear(hidden_dim * 2, num_classes)  # 双向 → hidden_dim*2\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, seq_len, num_features)\n",
        "        x = x.transpose(1, 2)  # (batch, features, seq_len)\n",
        "\n",
        "        # CNN\n",
        "        cnn_out = self.cnn(x)  # (batch, cnn_channels, seq_len)\n",
        "\n",
        "        # 送入DPGRU: (batch, seq_len, cnn_channels)\n",
        "        gru_input = cnn_out.transpose(1, 2)\n",
        "        gru_out, _ = self.gru(gru_input)\n",
        "\n",
        "        # 取最后一个时间步\n",
        "        last_output = gru_out[:, -1, :]  # (batch, hidden_dim*2)\n",
        "\n",
        "        # 分类\n",
        "        output = self.fc(self.dropout(last_output))\n",
        "        return output"
      ],
      "metadata": {
        "id": "pJIo5mhuN3O8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from opacus.layers import DPGRU, DPMultiheadAttention\n",
        "\n",
        "class SimpleSelfAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_heads=4):\n",
        "        super(SimpleSelfAttention, self).__init__()\n",
        "        assert hidden_dim % num_heads == 0, \"hidden_dim 必须能整除 num_heads\"\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = hidden_dim // num_heads\n",
        "\n",
        "        # Q, K, V 映射（Linear 支持 DP）\n",
        "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.k_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.v_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        self.out_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, hidden_dim)\n",
        "        B, T, H = x.size()\n",
        "\n",
        "        # 线性映射\n",
        "        Q = self.q_proj(x)  # (B, T, H)\n",
        "        K = self.k_proj(x)\n",
        "        V = self.v_proj(x)\n",
        "\n",
        "        # 分头\n",
        "        Q = Q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)  # (B, heads, T, head_dim)\n",
        "        K = K.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = V.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Attention score\n",
        "        scores = (Q @ K.transpose(-2, -1)) / (self.head_dim ** 0.5)  # (B, heads, T, T)\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "\n",
        "        # 聚合\n",
        "        out = attn @ V  # (B, heads, T, head_dim)\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, H)  # (B, T, H)\n",
        "\n",
        "        # 输出\n",
        "        return self.out_proj(out)\n",
        "\n",
        "class MultiAttn_GRU_Model(nn.Module):\n",
        "    def __init__(self, num_features, hidden_dim, num_classes,\n",
        "                 attn_heads=4, num_layers=1, dropout=0.3):\n",
        "        super(MultiAttn_GRU_Model, self).__init__()\n",
        "\n",
        "        # DP-GRU (单向)\n",
        "        self.gru = DPGRU(\n",
        "            input_size=num_features,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,       # DPGRU支持 batch_first\n",
        "            bidirectional=False\n",
        "        )\n",
        "        self.attn = SimpleSelfAttention(hidden_dim, attn_heads)\n",
        "\n",
        "        # 分类层\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 输入: (batch, seq_len, num_features)\n",
        "        # 先过GRU\n",
        "        gru_out, _ = self.gru(x)   # (batch, seq_len, hidden_dim)\n",
        "\n",
        "        # 注意力 (batch, seq_len, hidden_dim)\n",
        "        attn_out = self.attn(gru_out)\n",
        "\n",
        "        # 取最后时间步\n",
        "        last_output = attn_out[:, -1, :]\n",
        "\n",
        "        return self.fc(self.dropout(last_output))\n"
      ],
      "metadata": {
        "id": "UITIsArzQ0nv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the device\n",
        "print(f\"Using device: {device}\")\n",
        "num_features = X_train_feat.shape[1]   # DLC+Data[0-7] → 9\n",
        "hidden_dim = 128\n",
        "num_classes = len(set(y_train))\n",
        "\n",
        "model = MultiAttn_GRU_Model(num_features, hidden_dim, num_classes)\n",
        "model.to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hZvGO4bFesQ",
        "outputId": "8807cf8e-dadb-44f1-ce2a-fa79ec45c26e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from opacus import PrivacyEngine\n",
        "\n",
        "EPOCHS = 2\n",
        "privacy_engine = PrivacyEngine()\n",
        "model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
        "    module=model,\n",
        "    optimizer=optimizer,\n",
        "    data_loader=train_loader,\n",
        "    target_epsilon=5.0,\n",
        "    target_delta=1e-5,\n",
        "    epochs=EPOCHS,\n",
        "    max_grad_norm=1.0\n",
        ")"
      ],
      "metadata": {
        "id": "GsPXSeRba-um",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a40303ee-8d86-4cab-a0d7-5f95c56ac91f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:opacus.data_loader:Ignoring drop_last as it is not compatible with DPDataLoader.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from opacus import PrivacyEngine\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ==== 定义模型 ====\n",
        "class MultiAttGRU(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_heads, num_classes):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, batch_first=True)\n",
        "        self.attn = nn.MultiheadAttention(hidden_dim, num_heads, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.gru(x)  # [B, T, H]\n",
        "        attn_out, _ = self.attn(out, out, out)  # Self-Attention\n",
        "        pooled = attn_out.mean(dim=1)  # 平均池化\n",
        "        return self.fc(pooled)\n",
        "\n",
        "# ==== 训练函数 ====\n",
        "def train_and_eval(train_loader, test_loader, hidden_dim, num_heads, noise_multiplier, device=\"cuda\"):\n",
        "    model = MultiAttGRU(input_dim=1, hidden_dim=hidden_dim, num_heads=num_heads, num_classes=2).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    privacy_engine = PrivacyEngine()\n",
        "    model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
        "        module=model,\n",
        "        optimizer=optimizer,\n",
        "        data_loader=train_loader,\n",
        "        epochs=3,\n",
        "        target_epsilon=5,\n",
        "        target_delta=1e-5,\n",
        "        max_grad_norm=1.0,\n",
        "    )\n",
        "\n",
        "    for epoch in range(3):  # 小规模实验即可\n",
        "        model.train()\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(x)\n",
        "            loss = criterion(pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # ==== 测试 ====\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for x, y in test_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            pred = model(x)\n",
        "            y_true.extend(y.cpu().numpy())\n",
        "            y_pred.extend(pred.argmax(1).cpu().numpy())\n",
        "    return accuracy_score(y_true, y_pred), f1_score(y_true, y_pred)\n",
        "\n",
        "# ==== 超参数网格搜索 ====\n",
        "search_space = {\n",
        "    \"hidden_dim\": [64, 128, 256],\n",
        "    \"num_heads\": [2, 4, 8],\n",
        "    \"noise_multiplier\": [0.5, 1.0, 1.5]\n",
        "}\n",
        "\n",
        "results = []\n",
        "for hd in search_space[\"hidden_dim\"]:\n",
        "    for nh in search_space[\"num_heads\"]:\n",
        "        for nm in search_space[\"noise_multiplier\"]:\n",
        "            acc, f1 = train_and_eval(train_loader, test_loader, hd, nh, nm)\n",
        "            results.append((hd, nh, nm, acc, f1))\n",
        "            print(f\"hidden_dim={hd}, num_heads={nh}, noise={nm} => Acc={acc:.4f}, F1={f1:.4f}\")\n",
        "\n",
        "# ==== 可视化 ====\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(results, columns=[\"hidden_dim\", \"num_heads\", \"noise\", \"Acc\", \"F1\"])\n",
        "pivot = df.pivot_table(index=\"hidden_dim\", columns=\"noise\", values=\"Acc\")\n",
        "pivot.plot(kind=\"bar\", title=\"Accuracy vs Hyperparameters\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.show()\n",
        "\n",
        "print(\"最优参数：\")\n",
        "print(df.sort_values(by=\"F1\", ascending=False).head(1))\n"
      ],
      "metadata": {
        "id": "zXuBczd6TEUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9RH0DiP8TEP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O0vid5__TEL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lZBC88UNTD_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss, total_correct = 0, 0\n",
        "\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", leave=False)\n",
        "    for cids, feats, labels in progress_bar:\n",
        "        feats, labels = feats.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(feats)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        total_loss += loss.item() * labels.size(0)\n",
        "        total_correct += (preds == labels).sum().item()\n",
        "        progress_bar.set_postfix({'loss': total_loss/((progress_bar.n+1)*BATCH_SIZE), 'acc': total_correct/((progress_bar.n+1)*BATCH_SIZE)})\n",
        "\n",
        "    train_loss = total_loss / len(train_dataset)\n",
        "    train_acc  = total_correct / len(train_dataset)\n",
        "    print(f\"Epoch {epoch+1} → Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "\n",
        "\n",
        "    # -------------------------------\n",
        "    # 验证集\n",
        "    # -------------------------------\n",
        "    model.eval()\n",
        "    val_correct, val_total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for cids, feats, labels in val_loader:\n",
        "            feats, labels = feats.to(device), labels.to(device)\n",
        "            outputs = model(feats)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            val_correct += (preds == labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "    val_acc = val_correct / val_total\n",
        "\n",
        "    # 打印每个 epoch 结果\n",
        "    print(f\"Epoch {epoch+1} → Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # 获取详细的隐私账户信息\n",
        "    accountant = privacy_engine.accountant\n",
        "    # 方法1：获取当前epsilon值\n",
        "    epsilon = accountant.get_epsilon(delta=1e-5)\n",
        "    print(f\"11Current epsilon: {epsilon}\")\n",
        "    # 方法2：获取完整的隐私开销信息\n",
        "    privacy_spent = accountant.get_privacy_spent(delta=1e-5)\n",
        "    print(\"f222Privacy spent: {privacy_spent}\")\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# 测试集最终评估\n",
        "# -------------------------------\n",
        "model.eval()\n",
        "test_correct, test_total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for cids, feats, labels in test_loader:\n",
        "        feats, labels = feats.to(device), labels.to(device)\n",
        "        outputs = model(feats)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        test_correct += (preds == labels).sum().item()\n",
        "        test_total += labels.size(0)\n",
        "\n",
        "test_acc = test_correct / test_total\n",
        "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eZ7Ta9aN6m1",
        "outputId": "013088eb-b5b8-4302-c67c-a5a3f9b413be"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/2:   0%|          | 0/92 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1864: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 → Train Loss: 1.0467, Train Acc: 0.8310\n",
            "Epoch 1 → Train Loss: 1.0467, Train Acc: 0.8310, Val Acc: 0.8678\n",
            "11Current epsilon: 4.401611641339559\n",
            "f222Privacy spent: {privacy_spent}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 → Train Loss: 0.6996, Train Acc: 0.8574\n",
            "Epoch 2 → Train Loss: 0.6996, Train Acc: 0.8574, Val Acc: 0.8678\n",
            "11Current epsilon: 5.020429419685078\n",
            "f222Privacy spent: {privacy_spent}\n",
            "\n",
            "Final Test Accuracy: 0.8676\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5b08ee6"
      },
      "source": [
        "After installing the library, please run the code cell again."
      ]
    }
  ]
}